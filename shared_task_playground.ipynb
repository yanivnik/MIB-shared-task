{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import math\n",
    "import copy\n",
    "sys.path.append('EAP-IG/src')\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from eap.graph import Graph\n",
    "from eap.attribute import attribute\n",
    "from eap.attribute_node import attribute_node\n",
    "from eap.evaluate import evaluate_graph, evaluate_baseline\n",
    "from MIB_circuit_track.dataset import HFEAPDataset\n",
    "from MIB_circuit_track.metrics import get_metric\n",
    "from MIB_circuit_track.utils import MODEL_NAME_TO_FULLNAME, TASKS_TO_HF_NAMES\n",
    "from MIB_circuit_track.evaluation import evaluate_area_under_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\" # One of [\"gpt2\", \"llama3\", \"gemma2\", \"qwen2.5\"]\n",
    "model_name = MODEL_NAME_TO_FULLNAME[model_name]\n",
    "\n",
    "dataset_name = \"ioi\" # One of [\"ioi\", \"mcqa\", \"arithmetic_addition\", \"arithmetic_subtraction\", \"arc_easy\", \"arc_challenge\"]\n",
    "dataset_name = f\"mib-bench/{TASKS_TO_HF_NAMES[dataset_name]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8377083d508b42e582248c4c20f84fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ddaf7a5f19427a8eb42bb924b9ec1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aae57f412cf45c9b757baa172bff3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66de112fbfc40549b9bee27b23421c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59e5075badf4947a261bab5114cb199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34bab88775c486e91af37517072ac69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748c9d12c5b84556896f35777420021e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(model_name, attn_implementation=\"eager\", torch_dtype=torch.bfloat16, device=\"cuda\")\n",
    "model.cfg.use_split_qkv_input = True\n",
    "model.cfg.use_attn_result = True\n",
    "model.cfg.use_hook_mlp_in = True\n",
    "model.cfg.ungroup_grouped_query_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8f47d4791b4337aa1ed0e9a9fd0c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Old caching folder /home/yaniv.n/.cache/huggingface/datasets/mib-bench___ioi/default/0.0.0/e5f3468f3af4c0883be35cd3bced8c711c95d286 for dataset ioi exists but no data were found. Removing it. \n",
      "WARNING:datasets.builder:Old caching folder /home/yaniv.n/.cache/huggingface/datasets/mib-bench___ioi/default/0.0.0/e5f3468f3af4c0883be35cd3bced8c711c95d286 for dataset ioi exists but no data were found. Removing it. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed0f6713d29465faaf32a3980961521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/23.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e293251146e4a6aa4344c675f85e1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev.json:   0%|          | 0.00/23.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af33bf5966154898be814eb8068202fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.json:   0%|          | 0.00/2.77M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cff4aafa16443c7ae29629dc42e2969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92aa952f1e29410ba914bc0c95cc8bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03f53fb9d5e45699861a3db85ebe762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5058ea236195451791e52e870d403dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = HFEAPDataset(dataset_name, model.tokenizer)\n",
    "dataset.head(500)\n",
    "dataloader = dataset.to_dataloader(batch_size=64)\n",
    "metric_fn = get_metric(metric_name=\"logit_diff\", task=[\"ignore this\"], tokenizer=model.tokenizer, model=model)\n",
    "baseline = evaluate_baseline(model, dataloader, partial(metric_fn, loss=False, mean=False)).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution, Pruning, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.34s/it]\n",
      "100%|██████████| 8/8 [00:05<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define and attribute the graph in the following nodes\n",
    "\n",
    "g_edges = Graph.from_model(model) # For edge based attribution\n",
    "g_nodes = Graph.from_model(model) # For node based attribution\n",
    "\n",
    "attribute(model, g_edges, dataloader, partial(metric_fn, loss=True, mean=True), method='EAP-IG-inputs', ig_steps=5)\n",
    "attribute_node(model, g_nodes, dataloader, partial(metric_fn, loss=True, mean=True), method='EAP-IG-inputs', ig_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph given the calculated scores\n",
    "# Some relevant information:\n",
    "#   graph.in_graph and graph.nodes_in_graph need to be updated to reflect the edges or nodes that are in the graph\n",
    "#   graph.scores and graph.nodes_scores contain the scores of the edges or nodes, post attribution (scores will be non-zero only if attribution was done on edges, same for nodes_scores if attribution was done on nodes)\n",
    "\n",
    "def build_graph_from_attribution_scores(g_edges, g_nodes):\n",
    "    # NOTE: THIS FUNCTION SHOULD BE REPLACED TO BE BASED ON SOMETHING SMARTER (GRAPH ALGORITHM ETC)\n",
    "    g_edges.apply_greedy(n_edges=250, absolute=True, reset=True, prune=True)\n",
    "    g_nodes.apply_node_topn(n_nodes=45, absolute=True, reset=True, prune=True)\n",
    "    return g_edges\n",
    "\n",
    "graph = build_graph_from_attribution_scores(g_edges, g_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the graph to find it's faithfulness (higher = better)\n",
    "\n",
    "results = evaluate_graph(model, graph, dataloader, partial(metric_fn, loss=False, mean=False)).mean().item()\n",
    "print(f\"Faithfulness: {results / baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:07<00:00,  1.06it/s]\n",
      "100%|██████████| 8/8 [00:05<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.6405529953917051\n",
      "0.001 -0.6129032258064516\n",
      "0.002 -0.2269585253456221\n",
      "0.005 0.2569124423963134\n",
      "0.01 0.6405529953917051\n",
      "0.02 1.2258064516129032\n",
      "0.05 0.9769585253456221\n",
      "0.1 0.8847926267281107\n",
      "0.2 0.9585253456221198\n",
      "0.5 0.9815668202764977\n",
      "1.0 1.0\n",
      "AUC (Higher is better):  0.9687304147465438\n"
     ]
    }
   ],
   "source": [
    "# Similar to the two cells above, but calculates faithfulness \n",
    "# across percentages + calculates AUC (area under curve) for the faithfulness scores.\n",
    "# This is the score that is actually evaluated and important in the end.\n",
    " \n",
    "percentages = [0, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "\n",
    "def auc(faithfulnesses, log_scale=False, ):\n",
    "    area_under = 0.\n",
    "    area_from_1 = 0.\n",
    "    for i in range(len(faithfulnesses) - 1):\n",
    "        i_1, i_2 = i, i+1\n",
    "        x_1 = percentages[i_1]\n",
    "        x_2 = percentages[i_2]\n",
    "        # area from point to 100\n",
    "        if log_scale:\n",
    "            x_1 = math.log(x_1)\n",
    "            x_2 = math.log(x_2)\n",
    "        trapezoidal = (x_2 - x_1) * \\\n",
    "                        (((abs(1. - faithfulnesses[i_1])) + (abs(1. - faithfulnesses[i_2]))) / 2)\n",
    "        area_from_1 += trapezoidal \n",
    "        \n",
    "        trapezoidal = (x_2 - x_1) * ((faithfulnesses[i_1] + faithfulnesses[i_2]) / 2)\n",
    "        area_under += trapezoidal\n",
    "    average = sum(faithfulnesses) / len(faithfulnesses)\n",
    "\n",
    "    return area_under, area_from_1, average\n",
    "\n",
    "\n",
    "def build_graph_from_attribution_scores(g_edges, g_nodes, edge_percent, node_percent):\n",
    "    n_edges = int(len(g_edges.edges) * edge_percent)\n",
    "    n_nodes = min(len(g_nodes.nodes) - 1, max(0, int(len(g_nodes.nodes) * node_percent)))\n",
    "    g_edges.apply_greedy(n_edges=n_edges, absolute=True, reset=True, prune=True)\n",
    "\n",
    "    # Prune nodes based on node graph\n",
    "    g_nodes.apply_node_topn(n_nodes=n_nodes, absolute=True, reset=True, prune=True)\n",
    "    \n",
    "    g_edges.nodes_in_graph = copy.deepcopy(g_nodes.nodes_in_graph) # Take only nodes that were chosen also via node attribution\n",
    "    g_edges.in_graph[~g_edges.nodes_in_graph] = 0 # Zero out edges going out of nodes that were pruned out\n",
    "\n",
    "    n_edges_before = g_edges.in_graph.sum()\n",
    "    g_edges.prune()\n",
    "    n_edge_after = g_edges.in_graph.sum()\n",
    "\n",
    "    return g_edges\n",
    "\n",
    "\n",
    "g_edges = Graph.from_model(model) # For edge based attribution\n",
    "g_nodes = Graph.from_model(model) # For node based attribution\n",
    "attribute(model, g_edges, dataloader, partial(metric_fn, loss=True, mean=True), method='EAP-IG-inputs', ig_steps=5)\n",
    "attribute_node(model, g_nodes, dataloader, partial(metric_fn, loss=True, mean=True), method='EAP-IG-inputs', ig_steps=5)\n",
    "\n",
    "faithfulnesses = []\n",
    "for edge_percent in percentages:\n",
    "    node_percent = edge_percent\n",
    "    graph = build_graph_from_attribution_scores(g_edges, g_nodes, edge_percent, node_percent)\n",
    "    faith = evaluate_graph(model, graph, dataloader, partial(metric_fn, loss=False, mean=False), quiet=True).mean().item() / baseline\n",
    "    faithfulnesses.append(faith)\n",
    "    print(edge_percent, faithfulnesses[-1])\n",
    "\n",
    "print('AUC (Higher is better): ', auc(faithfulnesses, log_scale=False)[0])\n",
    "\n",
    "# Logging some output results:\n",
    "# Simple edge-based AUC: 0.9687\n",
    "# Edge-based + pruning nodes based on node scores (node_percent=edge_percent): 0.5644"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
